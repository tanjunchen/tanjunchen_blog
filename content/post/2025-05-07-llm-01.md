---
layout:     post
title:      "科普开源大模型基础知识"
subtitle:   "本篇文章主要介绍开源大模型的基础知识，如 LLama 4 和 Qwen 3 的核心亮点和基础架构"
description: ""
author: "陈谭军"
date: 2025-05-07
published: true
tags:
    - AI
    - 大模型
    - DeepSeek
categories:
    - TECHNOLOGY
showtoc: true
---

# Llama 4

北京时间2025年4月6日凌晨，Meta发布了外界期待许久的Llama4系列开源模型，目前它包括 Llama 4 Scout、Llama 4 Maverick、Llama 4 Behemoth。三种模型对应不同的使用需求，简单来说：
* Llama 4 Scout 是可以在单张 H100 上跑的多模态 MoE 模型；
* Llama 4 Maverick 是击败了 GPT-4o 和 Gemini 2.0，比 DeepSeek v3 小但编码和推理能力匹配的“最佳模型”；
* 还有一个即将发布的、隐藏在后为所有 Llama4 系列提供能力的 2880 亿活跃参数“巨兽”模型 Llama 4 Behemoth；

![](/images/2025-05-07-llm-01/1.png)

根据官方发布的介绍，此次 Llama4 有几个重要的技术亮点。
* MoE架构：此次是 Llama 首次采用混合专家架构，任务执行时仅激活部分参数（如Maverick总参数4000亿，活跃参数170亿），显著提升训练和推理效率；
* 多模态融合：早期融合（Early Fusion）策略统一处理文本、图像、视频，突破传统多模态模型的分阶段处理限制；
* 超长上下文：Scout支持1000万Token上下文窗口（约20K字文本或20小时视频），通过iRoPE架构实现“短序列训练，长序列泛化”；
* 部署上：Scout支持单张H100 GPU运行（Int4量化后），Maverick需H100 DGX集群，Behemoth则夸张地使用了32K块GPU训练；
* 后训练策略：采用“轻量级SFT → 在线RL → 轻量级DPO”流程，减少对齐约束，增强模型探索能力。引入“自我批判式数据筛选”，利用早期模型Check point检查点过滤低质量训练样本，提升最终性能。

在DeepSeek引发模型开源浪潮以前，Meta一直是开源模型的领先玩家及重要的行业奠基者。在ChatGPT横空出世7个多月后，Meta就率先宣布开源Llama 2，并且可免费商用。这也成为大模型发展的分水岭，是开源模型社区的历史性时刻。Llama第四代模型的发布，不仅是Meta应对DeepSeek等新兴开源势力的一次“回击”，同时也推动了开源模型技术的进一步发展和生态的进一步完善。

![](/images/2025-05-07-llm-01/2.png)

总结：由于 Behemoth 这个巨大参数的模型此次并没有正式发布，另外两个模型并没有太过让人震惊的突破 - 尤其在刷新评测榜单这件事已经没那么重要的今天，人们对Llama4的期待在于它的技术思路上是否有新玩意。主要亮点：Maverick和Scout模型此次也是首次采用了MoE混合专家结构，并且主打的亮点是原生的多模态能力、1千万上下文窗口。以上带有”戏谑性“的图片反应了大家对 Llama 4 重磅发布的反应，Meta AI 在大模型开源社区从领先者变回追赶者，或许有得忙了。

# Qwen 3

## 概述

![](/images/2025-05-07-llm-01/3.png)


2025年4月29日，Qwen 团队宣布推出 Qwen3，这是 Qwen 系列大型语言模型的最新成员。Qwen3 旗舰模型 Qwen3-235B-A22B 在代码、数学、通用能力等基准测试中，与 DeepSeek-R1、o1、o3-mini、Grok-3 和 Gemini-2.5-Pro 等顶级模型相比，表现出极具竞争力的结果。此外，小型 MoE 模型 Qwen3-30B-A3B 的激活参数数量是 QwQ-32B 的 10%，表现更胜一筹，甚至像 Qwen3-4B 这样的小模型也能匹敌 Qwen2.5-72B-Instruct 的性能。

![](/images/2025-05-07-llm-01/4.png)

![](/images/2025-05-07-llm-01/5.png)

Qwen 团队开源了两个 MoE 模型的权重：Qwen3-235B-A22B，一个拥有 2350 多亿总参数和 220 多亿激活参数的大模型，以及 Qwen3-30B-A3B，一个拥有约 300 亿总参数和 30 亿激活参数的小型 MoE 模型。此外，六个 Dense 模型也已开源，包括 Qwen3-32B、Qwen3-14B、Qwen3-8B、Qwen3-4B、Qwen3-1.7B 和 Qwen3-0.6B，均在 Apache 2.0 许可下开源。

![](/images/2025-05-07-llm-01/6.png)

## 核心亮点

### 多种思考模式（Qwen3 模型支持两种思考模式）
  * 思考模式：在这种模式下，模型会逐步推理，经过深思熟虑后给出最终答案。这种方法非常适合需要深入思考的复杂问题。
  * 非思考模式：在此模式中，模型提供快速、近乎即时的响应，适用于那些对速度要求高于深度的简单问题。

这种灵活性使用户能够根据具体任务控制模型进行“思考”的程度。例如，复杂的问题可以通过扩展推理步骤来解决，而简单的问题则可以直接快速作答，无需延迟。至关重要的是，这两种模式的结合大大增强了模型实现稳定且高效的“思考预算”控制能力。

![](/images/2025-05-07-llm-01/7.png)

![](/images/2025-05-07-llm-01/8.png)

### 多语言

Qwen3 模型支持 119 种语言和方言。这一广泛的多语言能力为国际应用开辟了新的可能性，让全球用户都能受益于这些模型的强大功能。

### 增强的 Agent 能力

Qwen 团队优化了 Qwen3 模型的 Agent 和代码能力，同时也加强了对 MCP 的支持。下面我们将提供一些示例，展示 Qwen3 是如何思考并与环境进行交互的。

## 预训练与后训练

预训练过程分为三个阶段。
* 在第一阶段（S1），模型在超过 30 万亿个 token 上进行了预训练，上下文长度为 4K token。这一阶段为模型提供了基本的语言技能和通用知识。
* 在第二阶段（S2），通过增加知识密集型数据（如 STEM、编程和推理任务）的比例来改进数据集，随后模型又在额外的 5 万亿个 token 上进行了预训练。
* 在第三阶段（S3），使用高质量的长上下文数据将上下文长度扩展到 32K token，确保模型能够有效地处理更长的输入。

![](/images/2025-05-07-llm-01/9.png)

在预训练方面，Qwen3 的数据集相比 Qwen2.5 有了显著扩展。Qwen2.5是在 18 万亿个 token 上进行预训练的，而 Qwen3 使用的数据量几乎是其两倍，达到了约 36 万亿个 token（DeepSeek-V3 模型在预训练阶段使用了约 14.8 万亿（14.8T）词元），涵盖了 119 种语言和方言。为了构建这个庞大的数据集，不仅从网络上收集数据，还从 PDF 文档中提取信息。使用 Qwen2.5-VL 从这些文档中提取文本，并用 Qwen2.5 改进提取内容的质量。为了增加数学和代码数据的数量，利用 Qwen2.5-Math 和 Qwen2.5-Coder 这两个数学和代码领域的专家模型合成数据，合成了包括教科书、问答对以及代码片段等多种形式的数据。后训练过程分为四个阶段。

![](/images/2025-05-07-llm-01/10.png)


Qwen 团队开发能够同时具备思考推理和快速响应能力的混合模型，实施了一个四阶段的训练流程。
该流程包括：（1）长思维链冷启动、（2）长思维链强化学习（3）思维模式融合、（4）通用强化学习。
* 在第一阶段，使用多样的的长思维链数据对模型进行了微调，涵盖了数学、代码、逻辑推理和 STEM 问题等多种任务和领域。这一过程旨在为模型配备基本的推理能力。
* 在第二阶段，重点是大规模强化学习，利用基于规则的奖励来增强模型的探索和钻研能力。
* 在第三阶段，在一份包括长思维链数据和常用的指令微调数据的组合数据上对模型进行微调，将非思考模式整合到思考模型中。确保了推理和快速响应能力的无缝结合。
* 在第四阶段，在包括指令遵循、格式遵循和 Agent 能力等在内的 20 多个通用领域的任务上应用了强化学习，以进一步增强模型的通用能力并纠正不良行为。

![](/images/2025-05-07-llm-01/11.png)

见解：Qwen3 代表了人类在通往通用人工智能（AGI）和超级人工智能（ASI）旅程中的一个重要里程碑。通过扩大预训练和强化学习的规模，实现了更高层次的智能。无缝集成了思考模式与非思考模式，为用户提供了灵活控制思考预算的能力。此外，还扩展了对多种语言的支持，帮助全球更多用户。
截至目前（2025.05.07），Qwen3（通义千问3）尚未发布完整的技术报告，多模态输入与输出能力、大规模训练数据与模型规模、推理能力的可控性、多语言等方面相对 DeepSeek R1 有较强的提升，但是里面的技术细节目前并没有完全披露。

# 实践案例

## 9.8 与 9.11 谁大？

![](/images/2025-05-07-llm-01/12.png)

![](/images/2025-05-07-llm-01/13.png)

![](/images/2025-05-07-llm-01/14.png)

![](/images/2025-05-07-llm-01/15.png)

![](/images/2025-05-07-llm-01/16.png)


## strawberry 有几个 r?

![](/images/2025-05-07-llm-01/17.png)

![](/images/2025-05-07-llm-01/18.png)

![](/images/2025-05-07-llm-01/19.png)

![](/images/2025-05-07-llm-01/20.png)

![](/images/2025-05-07-llm-01/21.png)

测试时间（2025.05.07），我们测试下大模型领域非常有意思的测试问题，如 9.8 与 9.11 谁大？strawberry 有几个 r？

9.8 与 9.11 谁大？这个问题，ChatGPT、DeepSeek、Qwen3、豆包、文心 4.5 Torbo 都回答正确；

strawberry 有几个 r ？这个问题，ChatGPT、豆包回答错误，DeepSeek、Qwen3、文心 4.5 Turbo 回答正确；

# 附录

* https://github.com/deepseek-ai/open-infra-index
* https://github.com/ByteDance-Seed/Seed-Thinking-v1.5/blob/main/seed-thinking-v1.5.pdf 
* https://qwenlm.github.io/blog/qwen3/
* https://cameronrwolfe.substack.com/p/language-model-training-and-inference
* [The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)
* https://medium.com/@chassweeting/the-state-of-gpt-by-andrew-kaparthy-fad2f007c1b9
* https://github.com/Jiayi-Pan/TinyZero
